{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "Follow the instructions below to get started!\n",
    "\n",
    "### Setup the Environment\n",
    "\n",
    "Run the next code cell to setup the environment.  This line might take a few minutes to run!\n",
    "\n",
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = 'local'   # set to udacity if running in the workspace\n",
    "if workspace == 'udacity':\n",
    "    !pip -q install ./python\n",
    "\n",
    "environments = {\n",
    "    'local-linux-agent': './unity_environments/Tennis_Linux/Tennis.x86', \n",
    "    'local-linux64-agent': './unity_environments/Tennis_Linux/Tennis.x86_64',         \n",
    "    'local-macos-agent': './unity_environments/Tennis.app',\n",
    "    'local-windows-agent': './unity_environments/Tennis_Windows_x86/Tennis.exe',\n",
    "    'local-windows64-agent': './unity_environments/single_agent/Tennis_Windows_x86_64/Tennis.exe',   \n",
    "    'udacity-agent': '/data/Tennis_Linux_NoVis/Tennis'\n",
    "}\n",
    "\n",
    "env_file_name = environments['local-macos-agent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Initialization\n",
    "\n",
    "Import all the packages required to run this notebook and setup the Unity Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from datetime import datetime\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from buffers.replaybuffer import ReplayBuffer\n",
    "from ddpg.agent import Agent as DDPG_Agent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = UnityEnvironment(file_name=env_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get required properties from the environment\n",
    "\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "The rewards for the agents looks like: [0.0, 0.0]\n",
      "The dones for the agents looks like: [False, False]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "# examine the rewards space \n",
    "rewards = env_info.rewards \n",
    "print('The rewards for the agents looks like:', rewards)\n",
    "\n",
    "# examine the done space \n",
    "dones = env_info.local_done \n",
    "print('The dones for the agents looks like:', dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent in the Environment\n",
    "\n",
    "This implementation using Deep Deterministic Policy Gradients (DDPG) algorithm to solve the problem.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "#### Agent\n",
    "\n",
    "The Agent uses the \"Actor-Critic\" that is a method which uses funcation approximation to learn a policy for any value function to solve the continuous action space. The Actor and Critic are 2 networks that do the following:\n",
    "1. Actor - Takes in a state and outputs the distribution over actions.\n",
    "2. Critic - Approximate the maximizer over the Q values of the next state that trains the action\n",
    "\n",
    "The Agent also uses a ReplyBuffer to interact with the environment for training. It uses the Eplison Greedy method to select the action with a starting epsilon of 1.0. The epsilon is decayed over the episodes using a decay rate of 0.995 with an ending epsilon of 0.01.\n",
    "   - Replay Buffer: It stores the experience that the agent observes, allowing us to reuse this data later. This allows the agent to sample the transistions randomly reducing the correlation in a batch. This has been shown to stablize and improve the DQN training. The buffer is a a cyclic buffer of bounded size that holds the experiences observed recently. It also implements a .sample() method for selecting a random batch of experiences for training. Each experience stored is actually a NamedTuple containing a single a named tuple representing a single interaction with the environment. It essentially maps (state, action) pairs to their (next_state, reward, done) result.\n",
    "   - Target network: This network is compared to the local network minimize the error better the 2 to maximize the reward to solve the task. The agent randomly initialized it at a start and it is updated everytime the buffer hits the batch size. The update is performed by picking random samples from the Replay Buffer to get the bast action for a given state.\n",
    "   - Local network: This network takes a state as input and outputs an action. It makes use of a Ornstein-Uhlenbeck process to perform exploration of actions during the training process\n",
    "\n",
    "It takes in the following parameters during contruction:\n",
    "1. device - Allows the training to switch between running on the CPU or GPU\n",
    "2. config dictinarary that contains the key values \n",
    "    - \"state_size\": dimension of each state\n",
    "    - \"action_size\": dimension of each action\n",
    "    - \"buffer_size\": replay buffer size\n",
    "    - \"batch_size\": minibatch size\n",
    "    - \"random_seed\": random seed\n",
    "    - \"gamma\": discount factor\n",
    "    - \"tau\": for soft update of target parameters\n",
    "    - \"lr_actor\": learning rate of the actor \n",
    "    - \"lr_critic\": learning rate of the critic\n",
    "    - \"weight_decay\": L2 weight decay\n",
    "    - \"learn_every\": learn from replay buffer every time step\n",
    "    - \"learn_batch_size\": number of batches to learn from replay buffer every learn_every time step\n",
    "    - \"grad_clip\": gradient value to clip at for critic\n",
    "    - \"eps_start\": starting value of epsilon, for epsilon-greedy action selection\n",
    "    - \"eps_end\": minimum value of epsilon\n",
    "    - \"eps_decay\": multiplicative factor (per episode) for decreasing epsilon\n",
    "    - \"print_every\": Print average every x episode,\n",
    "    - \"episode_steps\": Maximum number of steps to run for each episode\n",
    "    - \"mu\": mu for noise\n",
    "    - \"theta\": theta for noise \n",
    "    - \"sigma\": sigma for noise\n",
    "\n",
    "#### Contructor\n",
    "\n",
    "The constructor of the agent takes care of creating the Actor local and target network, Critic local and target network, an Adam optimizer, Ornstein-Uhlenbeck noise.\n",
    "\n",
    "#### Methods\n",
    "1. act\n",
    "This method is called with the current state which the agent uses with the Ornstein-Uhlenbeck noise process to explore the next action to perform.\n",
    "2. step\n",
    "This method takes in the `state, action, reward, next_state, done, timestep` values that were obtained from performing the action obtained from the act method previously. Internally it stores the inputs into the ReplayBuffer. And during every `learn_every` step it will trigger the learn method on a random sample of responses if the memory capacity is greater than the `batch_size`\n",
    "3. reset\n",
    "This method resets the noise process during learning\n",
    "4. learn_episode\n",
    "This method is called by the runner to learn from all time steps in an episode. The runner will call it when it finds the best episode average. It makes the network converge faster\n",
    "5. learn\n",
    "This method will calculate the target rewards and calculate the loss against the Q values in the local network using the `MSELoss` function. It will then perform any required optimization using the optimizer. And it will trigger the `soft_update` method. It also decays the eplison value that is used to generate the noise\n",
    "6. soft_update\n",
    "This method will perform an update on the parameters in the target network.\n",
    "\n",
    "#### Network\n",
    "\n",
    "##### Actor\n",
    "Forward pass Network that has 3 feed forward layers and a Batch Normalization Layer:\n",
    "1. fcs1 - in:state_size, out:fcs1_units\n",
    "2. bc1 - in:fcs1_units, out:fcs1_units\n",
    "3. relu: activation for adding nonlinearity\n",
    "4. fc2 - in:fcs1_units+action_size, out:fc2_units\n",
    "5. relu: activation for adding nonlinearity\n",
    "5. fc3: in:fc2_units, out:1\n",
    "\n",
    "##### Critic\n",
    "Forward pass Network mapping (Stae, action) pairs -> Q-values that has 3 feed forward layers and a Batch Normalization Layer:\n",
    "1. fcs1 - in:state_size, out:fc1_units\n",
    "2. bc1 - in:fc1_units, out:fc1_units\n",
    "3. relu: activation for adding nonlinearity\n",
    "4. fc2 - in:fc1_units, out:fc2_units\n",
    "5. relu: activation for adding nonlinearity\n",
    "5. fc3: in:fc2_units, out:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training 2019-11-17 16:59:41.425795\n",
      "Episode 20\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.043140\n",
      "Episode 40\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.036393\n",
      "Episode 60\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.042010\n",
      "Episode 80\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.046137\n",
      "Episode 100\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.041397\n",
      "Episode 120\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.048095\n",
      "Episode 140\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.041310\n",
      "Episode 160\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.035961\n",
      "Episode 180\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.038377\n",
      "Episode 200\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.054439\n",
      "Episode 220\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.031919\n",
      "Episode 240\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.042482\n",
      "Episode 260\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.047143\n",
      "Episode 280\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.049922\n",
      "Episode 300\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.051931\n",
      "Episode 320\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.042791\n",
      "Episode 340\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.043359\n",
      "Episode 360\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.057325\n",
      "Episode 380\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.043947\n",
      "Episode 400\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.048009\n",
      "Episode 420\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.042335\n",
      "Episode 440\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.041401\n",
      "Episode 460\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.037511\n",
      "Episode 480\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.042458\n",
      "Episode 500\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.045721\n",
      "Episode 520\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.042058\n",
      "Episode 540\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.031796\n",
      "Episode 560\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.051741\n",
      "Episode 580\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.041304\n",
      "Episode 600\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.053694\n",
      "Episode 620\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.041219\n",
      "Episode 640\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.047927\n",
      "Episode 660\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.034050\n",
      "Episode 680\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.031710\n",
      "Episode 700\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.045597\n",
      "Episode 720\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.045451\n",
      "Episode 740\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.039470\n",
      "Episode 760\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.042173\n",
      "Episode 780\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.046068\n",
      "Episode 800\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.047234\n",
      "Episode 820\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.032748\n",
      "Episode 840\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.036196\n",
      "Episode 860\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.051635\n",
      "Episode 880\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.040430\n",
      "Episode 900\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.048433\n",
      "Episode 920\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.042445\n",
      "Episode 940\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.034169\n",
      "Episode 960\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.051050\n",
      "Episode 980\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.048688\n",
      "Episode 1000\tLast 100 Episodes Average Scores: 0.00 took 0:00:00.041614\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = { \n",
    "    \"num_agents\": num_agents,\n",
    "    \"state_size\": state_size,    # dimension of each state\n",
    "    \"action_size\": action_size,  # dimension of each action\n",
    "    \"buffer_size\": int(1e10),    # replay buffer size\n",
    "    \"batch_size\": 256,           # minibatch size\n",
    "    \"random_seed\": 42,         # random seed\n",
    "    \"gamma\": 0.99,               # discount factor\n",
    "    \"tau\": 1e-3,                 # for soft update of target parameters\n",
    "    \"lr_actor\": 1e-4,            # learning rate of the actor \n",
    "    \"lr_critic\": 1e-3,           # learning rate of the critic\n",
    "    \"weight_decay\": 0,           # L2 weight decay\n",
    "    \"learn_every\": 20,           # learn from replay buffer every time step\n",
    "    \"learn_batch_size\": 10,      # number of batches to learn from replay buffer every learn_every time step\n",
    "    \"grad_clip\": 1.0,            # gradient value to clip at for critic\n",
    "    \"eps_start\": 1.0,            # starting value of epsilon, for epsilon-greedy action selection\n",
    "    \"eps_end\": 0,                # minimum value of epsilon\n",
    "    \"eps_decay\": 1e-8,           # multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"print_every\": 20,           # Print average every x episode,\n",
    "    \"mu\": 0,                     # mu for noise\n",
    "    \"theta\": 0.15,               # theta for noise \n",
    "    \"sigma\": 0.01                # sigma for noise\n",
    "}\n",
    "\n",
    "def ddpg(agent, n_episodes=1000):\n",
    "    best_score = 0\n",
    "    max_scores = []                               # Track best scores\n",
    "    last_100_max_scores = deque(maxlen=100)              # Best scores from most recent 100 episodes\n",
    "    print(f'Starting training {datetime.now()}')\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        start_datetime = datetime.now()\n",
    "        agent.reset_episode()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] \n",
    "        states = env_info.vector_observations \n",
    "        scores = np.zeros(num_agents)\n",
    "        timestep = 0\n",
    "        while True:\n",
    "            timestep += 1\n",
    "            actions = agent.act(states, add_noise=True)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            agent.step(states, actions, rewards, next_states, dones, timestep)\n",
    "\n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        max_score = np.max(scores)\n",
    "        max_scores.append(max_score)\n",
    "        last_100_max_scores.append(max_score)\n",
    "        if max_score > best_score:\n",
    "            best_score = max_score\n",
    "            agent.learn_best_episode()\n",
    "            \n",
    "        #print(f'\\rEpisode {i_episode}\\tScore: {max_score:.2f}\\tBest Score in all episodes: { best_score if best_score is not None else 0:.2f}', end=\"\")        \n",
    "        \n",
    "        mean_last_100_max_scores = np.mean(last_100_max_scores)\n",
    "        if mean_last_100_max_scores >= 0.5:\n",
    "            #print(f'\\nEnvironment solved in {i_episode-100:d} episodes!\\tAverage Score: { mean_last_100_max_scores:.2f}') \n",
    "            break\n",
    "        \n",
    "        if i_episode % config[\"print_every\"] == 0:\n",
    "            print(f'\\rEpisode {i_episode}\\tLast 100 Episodes Average Scores: {mean_last_100_max_scores:.2f} took {datetime.now() - start_datetime}')\n",
    "\n",
    "            \n",
    "    return max_scores, last_100_max_scores\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = DDPG_Agent(memory=ReplayBuffer(device=device, config=config), device=device, config=config)\n",
    "max_scores, last_100_max_scores = ddpg(agent, n_episodes=1000)    # The maximum step before the episode will end is 1,000\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(last_100_max_scores)), last_100_max_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Various hyperparameters were modified during the training sessions to find the sweet spot to make the agent perform better. The final run achieved the acheived the required running total of average score of 30 over 100 episodes. This run was solved from episode 619 to 719. The following was done in the implementation:\n",
    "\n",
    "1. A best_scores value was tracked in the running loop, when an episode exceeds the currect best. The runner will make the agent learn from all the steps in that episode. This make the agent train faster form the testing\n",
    "2. episode_steps was set to 1000, this was tested to be the maximum step before the environment will output a done which terminates the episode\n",
    "3. n_episodes was set to 1,000 this run was able to complete by 719, but from testing depending on the agents exploration during the run it might require to be a larger\n",
    "4. buffer_size was set to int(1e10), this consumes more memory but based the calculation of 719 X 1,000 = 719,000. I expanded it as I had a training run using int(1e10) that went into a local minimum since 100,000 will start ejecting older values after 100 episodes.\n",
    "5. sigma found that lowering this number which reduces the noise seem to improve the scores\n",
    "6. eps_decay this is set to a low number that adds more noise at the start to introduce randomness that encourages exploration. As the episodes grow it slowly decays and reduces the noise to reduce action exploration\n",
    "\n",
    "#### Ideas for Future Work\n",
    "1. A multi agent algorithm can be tested which is supposed to be more stable than DDPG\n",
    "2. Introducing Parameter noise that adds adaptive noise to the parameters of the neural network policy ranter than to its action space This injects randomness directly into the parameters of the agent, altering the types of decisions it makes such that they always fully depend on what the agent currently senses. It is suppose to help algorithms explore environments more effectively leading to higher scores and more elegant behaviours. (Reference -> https://openai.com/blog/better-exploration-with-parameter-noise/)\n",
    "3. Prioritized Experience Replay the implementation has a method that learns from the time steps every time it encounters an episode that beats the last best score found. More exploration into the Prioritized Experience Replay described by (https://arxiv.org/abs/1511.05952) to replay important transitions more frequently, and therefore learn more efficiently might be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
